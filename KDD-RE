Thank you for submitting your paper to KDD! I appreciate your effort to explore counterspeech generation, which is an important and underexplored topic. The paper proposes a simple and easy-to-follow method. However, as outlined in the weaknesses section, I have the following concerns:

The authors utilize two types of evaluation metrics: the first assesses the semantic and lexical quality of counterspeech, while the second relies on a score derived from JudgeLM. Are these metrics truly legitimate for evaluating the quality of counterspeech? How can we ensure there is no overlap between the two types? For instance, JudgeLM might already account for semantic and stylistic features. Moreover, if the main limitation of using JudgeLM is the potential for bias, as noted in the paper, then the complementary metrics should mainly focus on mitigating such bias. However, metrics like ROUGE-L and BLEU are not designed for that.
一方面：我们使用的几个评测方式是行业内大家都在用的普遍的counterspeech的度量，他们也被广泛用于各种生成任务，如文本摘要翻译等等，他们的评估的有效性也已经得到了认可。另一方面，我们也在3.2节做了人工评估，而人工评估的结果是和自动评估的结果一致的，即经过gen-ranking后的结果是大于baseline的结果，也进一步证明了本文使用的评测方法的有效性。



I also find that the inference objective is largely aligned with the evaluation metrics. For example, if we select candidates based on higher ROUGE-L and BLEU scores from a set of generated outputs, then naturally, those candidates will perform better under those same metrics. This evaluation setup seems unfair, it's like designing a quiz and then taking it yourself. A more rigorous approach would involve either using new, comprehensive, and convincing metrics, or having human annotators perform a thorough evaluation.

一方面，正如本文主要目标-旨在生成高质量的counterspeech，或者说对齐人工产生的counterspecch。ROUGE-L and BLEU BERT往往能通过单词覆盖率和语义相似度来效率对齐人工counterspecch。因此我们通过对齐这几个评估指标来找到更好的counterspeech是比较合理的。另一方面，使用完全由人工标注的会大大加大成本，这可能和自动生成counterspeech的目标相悖。



Regarding baselines, the paper currently only considers in-context learning (ICL) and chain-of-thought prompting. More diverse and relevant baselines, such as existing counterspeech generation models should also be included.

据我们所知，基于大模型的counterspeech生成主要是集中在使用ICL上，因此我们实现了这个baseline作为对比。而其他相关的工作，主要基于小模型的基础上，直接与我们的方法对比是不公平的，因此我们并没有将他们包含进来。



In Section 3.22 (Point-wise Scoring), if ground-truth scores can be directly calculated, why is there a need to train a predictor to approximate them?

我们在3.22计算ground-truth scores是构建predictor的训练集，用于对齐高质量的counterspeech分布。但在推理阶段，由于没有golden counterspeech是无法获得ground-truth scores的。而被训练好的predictor是reference-free的，在推理阶段在不提供reference的情况下，能找出高质量的

## Reviewer 2

Missing comparisons with RLHF-based methods (e.g., DPO, PPO, or rejection sampling), which are directly relevant for tasks involving response preference modeling.


No comparison to guided or constrained decoding techniques, which are standard for controlling generation quality and style — particularly important in safety-critical tasks like CS.

Q1，Q2，Q3，Q5，Q7
RLHF-based method是用于对齐偏好，主要用于模型生成内容的纠正，其并不能产生具体分数用于排序，因此和我们提出的方法相关性较少且无法比较。而我们的方法并未涉及任何的constrained decoding techniques，因此我们任务这个比较也是不合理的。而由于在multilingual的counterspeech领域训练集的稀缺性，去实现一个基于大模型的fine-tune也是不切实际的，事实上这类方法也是没有的。另外，如果在Counterspeech领域有使用constrained decoding techniques、RLHF-based、fine-tuned的方法的文献，也请您列出。


Generation is entirely unconstrained; diversity is achieved only via temperature and prompt variation, missing more sophisticated decoding methods.
我们ranking方法需要借助模型生成文本的多样性，来产生不同的结果，从而选择处更好的结果。如果进行解码限制，这一目的将很难达到。
The core contribution is a scoring-based selection, yet the generation process itself is relatively naïve and unchanged from basic prompting.

Only prompting-based baselines are considered; no strong fine-tuned generation model or retrieval-augmented generation baseline is included.
No iterative loop or feedback from ranking to generation — the method is static and lacks learning to improve over time.
我们对您的这个问题不太理解，我们并不需要iterative loop和feedback去生成。
Lack of ablations comparing re-ranking to reranking+finetuning, which would test if the selected outputs can improve future generations.
