Thank you for submitting your paper to KDD! I appreciate your effort to explore counterspeech generation, which is an important and underexplored topic. The paper proposes a simple and easy-to-follow method. However, as outlined in the weaknesses section, I have the following concerns:

The authors utilize two types of evaluation metrics: the first assesses the semantic and lexical quality of counterspeech, while the second relies on a score derived from JudgeLM. Are these metrics truly legitimate for evaluating the quality of counterspeech? How can we ensure there is no overlap between the two types? For instance, JudgeLM might already account for semantic and stylistic features. Moreover, if the main limitation of using JudgeLM is the potential for bias, as noted in the paper, then the complementary metrics should mainly focus on mitigating such bias. However, metrics like ROUGE-L and BLEU are not designed for that.
I also find that the inference objective is largely aligned with the evaluation metrics. For example, if we select candidates based on higher ROUGE-L and BLEU scores from a set of generated outputs, then naturally, those candidates will perform better under those same metrics. This evaluation setup seems unfair, it's like designing a quiz and then taking it yourself. A more rigorous approach would involve either using new, comprehensive, and convincing metrics, or having human annotators perform a thorough evaluation.
Regarding baselines, the paper currently only considers in-context learning (ICL) and chain-of-thought prompting. More diverse and relevant baselines, such as existing counterspeech generation models should also be included.
In Section 3.22 (Point-wise Scoring), if ground-truth scores can be directly calculated, why is there a need to train a predictor to approximate them?
