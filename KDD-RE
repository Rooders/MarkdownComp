Thank you for submitting your paper to KDD! I appreciate your effort to explore counterspeech generation, which is an important and underexplored topic. The paper proposes a simple and easy-to-follow method. However, as outlined in the weaknesses section, I have the following concerns:

The authors utilize two types of evaluation metrics: the first assesses the semantic and lexical quality of counterspeech, while the second relies on a score derived from JudgeLM. Are these metrics truly legitimate for evaluating the quality of counterspeech? How can we ensure there is no overlap between the two types? For instance, JudgeLM might already account for semantic and stylistic features. Moreover, if the main limitation of using JudgeLM is the potential for bias, as noted in the paper, then the complementary metrics should mainly focus on mitigating such bias. However, metrics like ROUGE-L and BLEU are not designed for that.
一方面：我们使用的几个评测方式是行业内大家都在用的普遍的counterspeech的度量，他们也被广泛用于各种生成任务，如文本摘要翻译等等，他们的评估的有效性也已经得到了认可。另一方面，我们也在3.2节做了人工评估，而人工评估的结果是和自动评估的结果一致的，即经过gen-ranking后的结果是大于baselin的结果。



I also find that the inference objective is largely aligned with the evaluation metrics. For example, if we select candidates based on higher ROUGE-L and BLEU scores from a set of generated outputs, then naturally, those candidates will perform better under those same metrics. This evaluation setup seems unfair, it's like designing a quiz and then taking it yourself. A more rigorous approach would involve either using new, comprehensive, and convincing metrics, or having human annotators perform a thorough evaluation.

Regarding baselines, the paper currently only considers in-context learning (ICL) and chain-of-thought prompting. More diverse and relevant baselines, such as existing counterspeech generation models should also be included.

In Section 3.22 (Point-wise Scoring), if ground-truth scores can be directly calculated, why is there a need to train a predictor to approximate them?
